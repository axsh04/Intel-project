{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23656b39-e424-4dfd-975b-288dcb234e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.5 huggingface-hub-0.33.0 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6191482a-740b-411f-b82e-c756b3d93776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from accelerate) (2.6.0+xpu)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from accelerate) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt==2025.0.2 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2025.0.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2025.0.2 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2025.0.2)\n",
      "Requirement already satisfied: intel-cmplr-lic-rt==2025.0.2 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2025.0.2)\n",
      "Requirement already satisfied: intel-sycl-rt==2025.0.2 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2025.0.2)\n",
      "Requirement already satisfied: tcmlib==1.2.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: umf==0.9.1 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.9.1)\n",
      "Requirement already satisfied: intel-pti==0.10.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.10.0)\n",
      "Requirement already satisfied: pytorch-triton-xpu==3.2.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch26/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2b5d5b-fba8-4800-8200-6b379e90196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"  # Paste your token here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8fee96a-34be-4827-bfde-52a74db03f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df03b49ce5d4833a33a503bdf86a979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LLaMA 3 Inference Ready. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " AI: Hi there! I'm your AI teacher today. How's it going? What would you like to learn about or practice today? We can have some fun with math, science, language arts, or anything else that interests you!\n",
      "\n",
      "Let me give you a few options:\n",
      "\n",
      "1. Explore the world of fractions: Learn how to add, subtract, multiply, and divide fractions with ease!\n",
      "2. Discover the magic of multiplication tables: Practice your times tables and become a multiplication master!\n",
      "3. Create your own stories: Use your imagination to craft exciting tales and characters, and even create your own comic strips!\n",
      "4. Solve puzzles and brain teasers: Challenge yourself with logic problems and riddles, and see if you can solve them all!\n",
      "\n",
      "Which one of these options sounds appealing to you?\n",
      "\n",
      "Type '1' for fractions, '2' for multiplication tables, '3' for storytelling, or '4' for puzzles. You can also type 'other' if none of these interest you, and we can come up with something else!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  can you explain photosynthesis with a diagram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " AI: Hi there!\n",
      "\n",
      "I'd be delighted to help you understand photosynthesis! It's a crucial process that occurs in plants, and it's actually quite fascinating.\n",
      "\n",
      "So, let me draw a simple diagram for you:\n",
      "\n",
      "**Step 1: Absorption**\n",
      "\n",
      "Imagine the plant's roots drinking water from the ground. The roots absorb water through tiny little holes called root hairs. At the same time, the plant's leaves spread out their edges like arms, absorbing carbon dioxide (CO2) from the air. This CO2 is like a special kind of food for the plant.\n",
      "\n",
      "**Step 2: Mixing**\n",
      "\n",
      "Now, imagine the plant taking this absorbed water and CO2, and mixing them together inside its leaves. This mixture is called the stoma (say \"sto-muh\"), which is like a tiny little factory inside the leaf. Here, enzymes help break down the CO2 into a type of sugar called glucose.\n",
      "\n",
      "**Step 3: Energy**\n",
      "\n",
      "As the plant breaks down the CO2, it also absorbs energy from sunlight. This energy is like a special kind of fuel that helps turn the CO2 into glucose. Think of it like a tiny little power station inside the leaf, converting sunlight into energy!\n",
      "\n",
      "**Step 4: Release**\n",
      "\n",
      "Finally, as the plant produces glucose, it releases oxygen (O2) back into the air. This oxygen is what we humans need to breathe to stay alive! It's like the plant is saying, \"Hey, I've got plenty of oxygen\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# llama3_inference_intel.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Model and tokenizer path\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  \n",
    "\n",
    "def load_model(model_id):\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,  # Recommended for Intel Gaudi or Xeon w/ AVX512\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c885bd07-754a-4587-9cf8-f9862be62eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a14eeb3d51c42419fbc704238821e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LLaMA 3 Inference Ready. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  can you generate a quiz for high school students regarding the topic photosynthesis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " AI: Hi there!\n",
      "\n",
      "I'd be happy to help you with a quiz on photosynthesis! Before we dive into it, let's quickly review what photosynthesis is.\n",
      "\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose (a type of sugar). This process occurs in specialized organelles called chloroplasts, which contain the pigment chlorophyll. Chlorophyll gives plants their green color and helps them absorb sunlight.\n",
      "\n",
      "Now, are you ready to test your knowledge? Here are 5 questions about photosynthesis:\n",
      "\n",
      "**Question 1:** What is the main source of energy for photosynthesis?\n",
      "\n",
      "A) Water\n",
      "B) Carbon dioxide\n",
      "C) Light energy from the sun\n",
      "D) Oxygen\n",
      "\n",
      "**Answer:** C) Light energy from the sun\n",
      "\n",
      "**Question 2:** Which part of the plant cell is responsible for photosynthesis?\n",
      "\n",
      "A) Nucleus\n",
      "B) Mitochondria\n",
      "C) Chloroplast\n",
      "D) Cell wall\n",
      "\n",
      "**Answer:** C) Chloroplast\n",
      "\n",
      "**Question 3:** What is the byproduct of photosynthesis that is released into the air?\n",
      "\n",
      "A) Glucose\n",
      "B) Oxygen\n",
      "C) Carbon dioxide\n",
      "D) Water vapor\n",
      "\n",
      "**Answer:** B) Oxygen\n",
      "\n",
      "**Question 4:** What is the reactant that plants use during photosynthesis?\n",
      "\n",
      "A) Carbon dioxide\n",
      "B) Water\n",
      "C) Nitrogen\n",
      "D) Phosphorus\n",
      "\n",
      "**Answer\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt, tokenizer, model):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def main():\n",
    "    tokenizer, model = load_model(model_id)\n",
    "    print(\"\\n LLaMA 3 Inference Ready. Type 'exit' to quit.\")\n",
    "    system_prompt = \"You are an AI classroom teacher. Explain concepts in a friendly, simple, and engaging way.\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.strip().lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{user_input}\\n<|assistant|>\"\n",
    "        response = generate_response(prompt, tokenizer, model)\n",
    "\n",
    "        # Clean and show assistant reply\n",
    "        if \"<|assistant|>\" in response:\n",
    "            response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "        print(f\"\\n AI: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ad5989-3864-44f5-85e4-612bcdeeb41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: The \"no new privileges\" flag is set, which prevents sudo from running as root.\n",
      "sudo: If sudo is running in a container, you may need to adjust the container configuration to disable the flag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: The \"no new privileges\" flag is set, which prevents sudo from running as root.\n",
      "sudo: If sudo is running in a container, you may need to adjust the container configuration to disable the flag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d62f4-59a4-4588-bbc1-00b4f1f47b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.6",
   "language": "python",
   "name": "pytorch26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
